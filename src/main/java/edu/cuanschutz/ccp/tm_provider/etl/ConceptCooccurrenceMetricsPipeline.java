package edu.cuanschutz.ccp.tm_provider.etl;

import java.util.Map;
import java.util.logging.Level;
import java.util.logging.Logger;
import java.util.stream.StreamSupport;

import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.io.Compression;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.Description;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.GroupByKey;
import org.apache.beam.sdk.transforms.Keys;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.Sum;
import org.apache.beam.sdk.transforms.View;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.PCollection;
import org.apache.beam.sdk.values.PCollectionView;
import org.apache.commons.codec.digest.DigestUtils;

import com.google.common.annotations.VisibleForTesting;

import edu.cuanschutz.ccp.tm_provider.etl.fn.ConceptCooccurrenceCountsFn;
import edu.cuanschutz.ccp.tm_provider.etl.fn.ConceptCooccurrenceCountsFn.ConceptPair;
import edu.cuanschutz.ccp.tm_provider.etl.fn.PCollectionUtil;
import edu.cuanschutz.ccp.tm_provider.etl.fn.PCollectionUtil.Delimiter;

/**
 * Computes normalized google distance between concepts. Uses files generated by
 * the {@link ConceptCooccurrenceCountsPipeline}.
 */
public class ConceptCooccurrenceMetricsPipeline {

	private static final Logger LOGGER = Logger.getLogger(ConceptCooccurrenceMetricsPipeline.class.getName());
	protected static final String KGX_OUTPUT_EDGE_LABEL = "biolink:related_to";
	protected static final String KGX_OUTPUT_RELATION = "SIO:000001"; // 'is related to'
	protected static final String KGX_ASSOCIATION_TYPE = "biolink:Association";

	public interface Options extends DataflowPipelineOptions {

		@Description("File pattern to match the files containing concept-id/document-id pairs")
		String getSingletonFilePattern();

		void setSingletonFilePattern(String filePattern);

		@Description("File pattern to match the files containing concept-id-pair/document-id pairs")
		String getPairFilePattern();

		void setPairFilePattern(String filePattern);

		@Description("File pattern to match the files containing document-id/concept-count pairs")
		String getConceptCountFilePattern();

		void setConceptCountFilePattern(String filePattern);

		@Description("File pattern to match the files containing concept-id to label pairs")
		String getLabelMapFilePattern();

		void setLabelMapFilePattern(String filePattern);

		@Description("Delimiter used in the id-to-label file")
		Delimiter getLabelMapFileDelimiter();

		void setLabelMapFileDelimiter(Delimiter delimiter);

		@Description("File pattern to match the files containing concept-id to biolink category pairs")
		String getCategoryMapFilePattern();

		void setCategoryMapFilePattern(String filePattern);

		@Description("Delimiter used in the id-to-biolink-category file")
		Delimiter getCategoryMapFileDelimiter();

		void setCategoryMapFileDelimiter(Delimiter delimiter);

		@Description("Path to the bucket where results will be written")
		String getOutputBucket();

		void setOutputBucket(String bucketPath);

	}

	public static void main(String[] args) {
		Options options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);
		Pipeline p = Pipeline.create(options);

		final PCollection<KV<String, Long>> conceptIdToCounts = getSingletonCountMapView(options.getSingletonFilePattern(), p);

		final PCollectionView<Map<String, Long>> singletonCountMap = conceptIdToCounts
				.apply(View.<String, Long>asMap());

		final PCollectionView<Long> totalConceptCount = getTotalConceptCountView(options, p);

		final PCollectionView<Map<String, String>> conceptIdToLabelMap = PCollectionUtil.fromTwoColumnFiles("label map",p,
				options.getLabelMapFilePattern(), options.getLabelMapFileDelimiter(), Compression.GZIP)
				.apply(View.<String, String>asMap());

		final PCollectionView<Map<String, String>> conceptIdToCategoryMap = PCollectionUtil.fromTwoColumnFiles("category map",p,
				options.getCategoryMapFilePattern(), options.getCategoryMapFileDelimiter(), Compression.GZIP)
				.apply(View.<String, String>asMap());

		// get concept pairs mapped to document identifiers
		PCollection<KV<String, String>> pairToDocId = PCollectionUtil.fromTwoColumnFiles("pair file",p,
				options.getPairFilePattern(), ConceptCooccurrenceCountsFn.OUTPUT_FILE_DELIMITER,
				Compression.UNCOMPRESSED);
		PCollection<KV<String, Iterable<String>>> pairToDocIds = pairToDocId.apply("group-by-concept-id",
				GroupByKey.<String, String>create());

		PCollection<KV<String, Double>> pairToNgd = getConceptIdPairToNgdScore(singletonCountMap, totalConceptCount,
				pairToDocIds);

		// create KGX node lines
		PCollection<String> nodeTsv = createKgxNodeLines(conceptIdToCounts, conceptIdToLabelMap,
				conceptIdToCategoryMap);

		// create KGX edge lines
		PCollection<String> edgeTsv = createKgxEdgeLines(conceptIdToLabelMap, conceptIdToCategoryMap, pairToNgd);

		// output kgx node/edge files
		nodeTsv.apply("write KGX node file", TextIO.write().to(options.getOutputBucket()).withSuffix(".nodes.tsv"));

		edgeTsv.apply("write KGX edge file", TextIO.write().to(options.getOutputBucket()).withSuffix(".edges.tsv"));

		p.run().waitUntilFinish();
	}

	private static PCollection<KV<String, Double>> getConceptIdPairToNgdScore(
			final PCollectionView<Map<String, Long>> singletonCountMap, final PCollectionView<Long> totalConceptCount,
			PCollection<KV<String, Iterable<String>>> pairToDocIds) {
		PCollection<KV<String, Double>> pairToNgd = pairToDocIds
				.apply(ParDo.of(new DoFn<KV<String, Iterable<String>>, KV<String, Double>>() {
					private static final long serialVersionUID = 1L;

					@ProcessElement
					public void processElement(ProcessContext c) {
						KV<String, Iterable<String>> element = c.element();
						ConceptPair pair = ConceptPair.fromReproducibleKey(element.getKey());
						long N = c.sideInput(totalConceptCount);
						Map<String, Long> conceptCountMap = c.sideInput(singletonCountMap);

						Long fx = conceptCountMap.get(pair.getConceptId1());
						Long fy = conceptCountMap.get(pair.getConceptId2());
						Long fxy = StreamSupport.stream(element.getValue().spliterator(), false).count();

						if (fx == null) {
							LOGGER.log(Level.WARNING,
									String.format("Unable to find concept count for id: %s", pair.getConceptId1()));
						}
						if (fy == null) {
							LOGGER.log(Level.WARNING,
									String.format("Unable to find concept count for id: %s", pair.getConceptId2()));
						}

						if (fx != null && fy != null) {
							double ngd = normalizedGoogleDistance(N, fx, fy, fxy);
							c.output(KV.of(element.getKey(), ngd));
						}
					}

				}).withSideInputs(singletonCountMap, totalConceptCount));
		return pairToNgd;
	}

	private static PCollection<String> createKgxEdgeLines(
			final PCollectionView<Map<String, String>> conceptIdToLabelMap,
			final PCollectionView<Map<String, String>> conceptIdToCategoryMap,
			PCollection<KV<String, Double>> pairToNgd) {
		PCollection<String> edgeTsv = pairToNgd.apply(ParDo.of(new DoFn<KV<String, Double>, String>() {
			private static final long serialVersionUID = 1L;

			@ProcessElement
			public void processElement(ProcessContext c) {
				KV<String, Double> pairToNgd = c.element();

				String pairKey = pairToNgd.getKey();
				ConceptPair pair = ConceptPair.fromReproducibleKey(pairKey);

				String subjectId = pair.getConceptId1();
				String objectId = pair.getConceptId2();
				String edgeLabel = KGX_OUTPUT_EDGE_LABEL;
				String relation = KGX_OUTPUT_RELATION;
				String associationType = KGX_ASSOCIATION_TYPE;
				Double ngd = pairToNgd.getValue();
				String associationId = DigestUtils.sha256Hex(pairKey);

				String kgxEdgeStr = String.format("%s\t%s\t%s\t%s\t%s\t%s\t%s", subjectId, edgeLabel, objectId,
						relation, associationId, associationType, ngd);

				c.output(kgxEdgeStr);
			}

		}).withSideInputs(conceptIdToLabelMap, conceptIdToCategoryMap));
		return edgeTsv;
	}

	private static PCollection<String> createKgxNodeLines(final PCollection<KV<String, Long>> conceptIdToCounts,
			final PCollectionView<Map<String, String>> conceptIdToLabelMap,
			final PCollectionView<Map<String, String>> conceptIdToCategoryMap) {
		PCollection<String> nodeTsv = conceptIdToCounts.apply(Keys.<String>create())
				.apply(ParDo.of(new DoFn<String, String>() {
					private static final long serialVersionUID = 1L;

					@ProcessElement
					public void processElement(ProcessContext c) {
						String conceptId = c.element();
						Map<String, String> conceptLabelMap = c.sideInput(conceptIdToLabelMap);
						Map<String, String> conceptCategoryMap = c.sideInput(conceptIdToCategoryMap);

						String label = conceptLabelMap.get(conceptId);
						String category = conceptCategoryMap.get(conceptId);

						if (label == null) {
							label = "UKNOWN";
						}
						if (category == null) {
							category = "UKNOWN";
						}
						String kgxNodeStr = String.format("%s\t%s\t%s", conceptId, label, category);

						c.output(kgxNodeStr);
					}

				}).withSideInputs(conceptIdToLabelMap, conceptIdToCategoryMap));
		return nodeTsv;
	}

	private static PCollectionView<Long> getTotalConceptCountView(Options options, Pipeline p) {
		// compute the total number of concepts observed (N)
		PCollection<KV<String, String>> docIdToConceptCountStr = PCollectionUtil.fromTwoColumnFiles("concept counts",p,
				options.getConceptCountFilePattern(), ConceptCooccurrenceCountsFn.OUTPUT_FILE_DELIMITER,
				Compression.UNCOMPRESSED);
		PCollection<KV<String, Long>> docIdToConceptCount = docIdToConceptCountStr
				.apply(ParDo.of(new DoFn<KV<String, String>, KV<String, Long>>() {
					private static final long serialVersionUID = 1L;

					@ProcessElement
					public void processElement(ProcessContext c) {
						KV<String, String> element = c.element();
						c.output(KV.of(element.getKey(), Long.parseLong(element.getValue())));
					}
				}));
		// dedup in case some documents got processed multiple times
		PCollection<Long> conceptCounts = PipelineMain.deduplicateByKey(docIdToConceptCount);
		final PCollectionView<Long> totalConceptCount = conceptCounts.apply(Sum.longsGlobally().asSingletonView());
		return totalConceptCount;
	}

	public static PCollection<KV<String, Long>> getSingletonCountMapView(String singletonFilePattern, Pipeline p) {
		// get lines that link concept identifiers to content identifiers (could be a
		// document id, but could also be a sentence id, or something else).
		PCollection<KV<String, String>> conceptIdToDocId = PCollectionUtil.fromTwoColumnFiles("singletons",p,
				singletonFilePattern, ConceptCooccurrenceCountsFn.OUTPUT_FILE_DELIMITER,
				Compression.UNCOMPRESSED);
		// group by concept-id so that we now map from concept-id to all of its
		// content-ids
		PCollection<KV<String, Iterable<String>>> conceptIdToDocIds = conceptIdToDocId.apply("group-by-concept-id",
				GroupByKey.<String, String>create());
		// return mapping of concept id to the number of documents (or sentences, etc.)
		// in which it was observed
		PCollection<KV<String, Long>> conceptIdToCounts = conceptIdToDocIds
				.apply(ParDo.of(new DoFn<KV<String, Iterable<String>>, KV<String, Long>>() {
					private static final long serialVersionUID = 1L;

					@ProcessElement
					public void processElement(ProcessContext c) {
						KV<String, Iterable<String>> element = c.element();

						long count = StreamSupport.stream(element.getValue().spliterator(), false).count();

						c.output(KV.of(element.getKey(), count));
					}
				}));

		return conceptIdToCounts;
	}

	@VisibleForTesting
	protected static double normalizedGoogleDistance(long N, long fx, long fy, long fxy) {
		double logFx = Math.log10((double) fx);
		double logFy = Math.log10((double) fy);
		double logFxy = Math.log10((double) fxy);
		double logN = Math.log10((double) N);

		double ngd = (Math.max(logFx, logFy) - logFxy) / (logN - Math.min(logFx, logFy));
		return ngd;
	}

}
