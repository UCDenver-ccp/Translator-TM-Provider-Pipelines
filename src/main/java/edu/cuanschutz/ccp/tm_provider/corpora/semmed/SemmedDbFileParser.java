package edu.cuanschutz.ccp.tm_provider.corpora.semmed;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Reader;
import java.io.StringReader;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.zip.GZIPInputStream;

import com.opencsv.CSVReader;

import edu.ucdenver.ccp.common.collections.CollectionsUtil;
import edu.ucdenver.ccp.common.file.CharacterEncoding;
import edu.ucdenver.ccp.common.file.FileWriterUtil;
import edu.ucdenver.ccp.common.file.reader.Line;
import edu.ucdenver.ccp.common.file.reader.StreamLineIterator;

/**
 * Colab notebook:
 * https://colab.research.google.com/drive/16AcQQ4AaiYG17zWxGDijXEiU8X889xbZ#scrollTo=tYFT6ZIflXtE
 * 
 * parses an export of semmed that joins tables. Note that the predication table
 * is a custom version that was provided by BTE.
 *
 * <pre>
 * SELECT *
     FROM SEMMEDDB_SENTENCE s
     INNER JOIN SEMMEDDB_PREDICATION p ON s.SENTENCE_ID = p.SENTENCE_ID
     INNER JOIN SEMMEDDB_PREDICATION_AUX a ON p.PREDICATION_ID = a.PREDICATION_ID
     WHERE p.PREDICATE = "TREATS" 
      AND p.SUBJECT_SEMTYPE in ("aapp", "antb", "bacs", "bodm", "chem", "chvf", "chvs", "clnd", "elii", "enzy", "hops", "horm", "imft", "irda", "inch", "nnon", "orch", "phsu", "rcpt", "vita", "diap", "edac", "hlca", "lbpr", "mbrt", "resa", "topp", "medd", "drdd", "resd")
      AND p.OBJECT_SEMTYPE in ("acab", "anab", "comd", "cgab", "dsyn", "emod", "fndg", "inpo", "mobd", "neop", "patf", "sosy", "virs", "fngs", "bact")
 * </pre>
 */
public class SemmedDbFileParser {

	// @formatter:off
	/* header */
	/* 0: SENTENCE_ID 	
	 * 1: PMID 	
	 * 2: TYPE 	
	 * 3: NUMBER 	
	 * 4: SENT_START_INDEX 	
	 * 5: SENTENCE 	
	 * 6: SENT_END_INDEX 	
	 * 7: SECTION_HEADER 	
	 * 8: NORMALIZED_SECTION_HEADER 	
	 * 9: BTE_ID 	
	 * 10: PREDICATION_ID 	
	 * 11: SENTENCE_ID 	
	 * 12: PMID 	
	 * 13: PREDICATE 	
	 * 14: SUBJECT_CUI 	
	 * 15: SUBJECT_NAME
	 * 16: SUBJECT_SEMTYPE 	
	 * 17: SUBJECT_NOVELTY 	
	 * 18: OBJECT_CUI 	
	 * 19: OBJECT_NAME 	
	 * 20: OBJECT_SEMTYPE 	
	 * 21: OBJECT_NOVELTY 	
	 * 22: SUBJECT_PREFIX 	
	 * 23: OBJECT_PREFIX 	
	 * 24: PREDICATION_AUX_ID 	
	 * 25: PREDICATION_ID 	
	 * 26: SUBJECT_TEXT 	
	 * 27: SUBJECT_DIST 	
	 * 28: SUBJECT_MAXDIST 	
	 * 29: SUBJECT_START_INDEX 	
	 * 30: SUBJECT_END_INDEX 	
	 * 31: SUBJECT_SCORE 	
	 * 32: INDICATOR_TYPE 	
	 * 33: PREDICATE_START_INDEX 	
	 * 34: PREDICATE_END_INDEX 	
	 * 35: OBJECT_TEXT 	
	 * 36: OBJECT_DIST 	
	 * 37: OBJECT_MAXDIST 	
	 * 38: OBJECT_START_INDEX 	
	 * 39: OBJECT_END_INDEX 	
	 * 40: OBJECT_SCORE 	
	 * 41: CURR_TIMESTAMP */
	// @formatter:on

	public static void main(String[] args) {
		File file = new File(
				"/Users/bill/projects/ncats-translator/relay_feb_2023/from_db/semmed_treats_sentences.csv.gz");

		// derived from a file created by Andrew Su
		File edgePmidsFile = new File(
				"/Users/bill/projects/ncats-translator/relay_feb_2023/edge_pmidlist.TREATS.tsv.gz");

		// representative sample of 5000
		File sampleFile = new File("/Users/bill/projects/ncats-translator/relay_feb_2023/from_db/treats_sample.csv");

		File interveningTokenCountFile = new File(
				"/Users/bill/projects/ncats-translator/relay_feb_2023/from_db/intervening_token_count_treats.csv");
		File errorFile = new File(
				"/Users/bill/projects/ncats-translator/relay_feb_2023/from_db/intervening_token_count_treats.errors.csv");

		try {
//			getSemanticTypeMatrix(file);

			Set<String> samplePredicationIds = loadSamplePredicationIds(sampleFile);

			/*
			 * set sample predication Ids to null to do the heatmap for all; otherwise it
			 * will be a heatmap for the sample
			 */
//			Set<String> samplePredicationIds = null;
			Map<String, Integer> cuiKeyToPmidCount = getSubjObjCuiToPmidCountMap(edgePmidsFile);
			System.out.println("\n\n\n\n");
			getSubjectObjectDistanceDistribution(file, interveningTokenCountFile, errorFile, cuiKeyToPmidCount,
					samplePredicationIds);
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	private static Set<String> loadSamplePredicationIds(File sampleFile) throws IOException {
		Set<String> predicationIds = new HashSet<String>();
		for (StreamLineIterator lineIter = new StreamLineIterator(sampleFile, CharacterEncoding.UTF_8); lineIter
				.hasNext();) {
			Line line = lineIter.next();
			if (line.getLineNumber() == 0) {
				// skip header
				continue;
			}
			// predication id is the second column
			String id = line.getText().split(",")[1];
			predicationIds.add(id);
		}
		return predicationIds;
	}

	/**
	 * Parses a file generated by Andrew Su that lists pmids for each assertion
	 * 
	 * Header:
	 * 
	 * <pre>
	 * PREDICATE	SUBJECT_CUI	SUBJECT_NAME	SUBJECT_SEMTYPE	OBJECT_CUI	OBJECT_NAME	OBJECT_SEMTYPE	PMIDS
	 * </pre>
	 * 
	 * @param edgePmidsFile
	 * @return
	 * @throws IOException
	 * @throws FileNotFoundException
	 */
	public static Map<String, Integer> getSubjObjCuiToPmidCountMap(File edgePmidsFile)
			throws FileNotFoundException, IOException {
		Map<String, Integer> map = new HashMap<String, Integer>();

		for (StreamLineIterator lineIter = new StreamLineIterator(
				new GZIPInputStream(new FileInputStream(edgePmidsFile)), CharacterEncoding.UTF_8, null); lineIter
						.hasNext();) {
			Line line = lineIter.next();

			if (line.getLineNumber() % 10000 == 0) {
				System.out.println("retrieving pmid counts progress: " + line.getLineNumber());
			}

			String[] cols = line.getText().split("\\t");

			String predicate = cols[0];

			if (predicate.equals("TREATS")) {
				String subjCui = cols[1];
				String objCui = cols[4];
				String pmids = cols[7];

				int pmidCount = pmids.split(",").length;
				String key = createSubjObjCuiKey(subjCui, objCui);

				// the key is not necessarily unique as there are predications in the file that
				// differ only in the semantic types of the subject and object. We will combine
				// them by adding pmid counts where only the semantic types differ.

				CollectionsUtil.addToCountMap(key, pmidCount, map);

//				if (!map.containsKey(key)) {
//					map.put(key, pmidCount);
//				} else {
//					throw new IllegalArgumentException("Observed duplicate key: " + key);
//				}
			}

		}

		return map;
	}

	private static String createSubjObjCuiKey(String subjCui, String objCui) {
		return String.format("%s_%s", subjCui, objCui);
	}

	/**
	 * Count the number of tokens between the subject and object entity
	 * 
	 * @param inputFile
	 * @param cuiKeyToPmidCount
	 * @param samplePredicationIds
	 * @throws FileNotFoundException
	 * @throws IOException
	 */
	public static void getSubjectObjectDistanceDistribution(File inputFile, File outputFile, File errorFile,
			Map<String, Integer> cuiKeyToPmidCount, Set<String> samplePredicationIds)
			throws FileNotFoundException, IOException {

		Set<String> subjSemTypes = new HashSet<String>();
		Set<String> objSemTypes = new HashSet<String>();
		Map<String, Integer> keyToCountMap = new HashMap<String, Integer>();

		try (BufferedWriter writer = FileWriterUtil.initBufferedWriter(outputFile);
				BufferedWriter errWriter = FileWriterUtil.initBufferedWriter(errorFile)) {
			writer.write(String.format("%s,%s,%s,%s,%s,%s,%s\n", "predication_id", "intervening_token_count",
					"sentence_token_count", "pmid_count", "sem_types", "pmid_count_bucket",
					"intervening_token_count_bucket"));
			for (StreamLineIterator lineIter = new StreamLineIterator(
					new GZIPInputStream(new FileInputStream(inputFile)), CharacterEncoding.UTF_8, null); lineIter
							.hasNext();) {
				Line line = lineIter.next();
				if (line.getLineNumber() % 10000 == 0) {
					System.out.println("progress: " + line.getLineNumber());
				}

				String[] nextRecord = null;
				try {
					Reader lineReader = new StringReader(line.getText());
					CSVReader csvReader = new CSVReader(lineReader);
					nextRecord = csvReader.readNext();
					csvReader.close();
				} catch (Exception e) {
					errWriter.write(String.format("CSV parse error. Line %d\n", line.getLineNumber()));
					continue;
				}
				String predicationId = nextRecord[10];

				try {
//					System.out.println(nextRecord[0] + " -- " + line.getLineNumber() + " -- " + predicationId);
//					if (line.getLineNumber() > 10000) {
//						break;
//					}

					int sentenceStartIndex = Integer.parseInt(nextRecord[4]);
//					int sentenceEndIndex = Integer.parseInt(nextRecord[6]);
					String sentenceText = nextRecord[5];
					int subjStartIndex = Integer.parseInt(nextRecord[29]) - sentenceStartIndex;
					int subjEndIndex = Integer.parseInt(nextRecord[30]) - sentenceStartIndex;
					int objStartIndex = Integer.parseInt(nextRecord[38]) - sentenceStartIndex;
					int objEndIndex = Integer.parseInt(nextRecord[39]) - sentenceStartIndex;

					String subjectText = nextRecord[26];
					String objectText = nextRecord[35];
					String subjectCui = nextRecord[14];
					String objectCui = nextRecord[18];
					String subjSemType = nextRecord[16];
					String objSemType = nextRecord[20];

					int segmentStart = subjEndIndex;
					int segmentEnd = objStartIndex;
					if (objStartIndex < subjStartIndex) {
						segmentStart = objEndIndex;
						segmentEnd = subjStartIndex;
					}

					String observedSubjectText = sentenceText.substring(subjStartIndex, subjEndIndex);
					String observedObjectText = sentenceText.substring(objStartIndex, objEndIndex);

					if (!subjectText.equals(observedSubjectText) || !objectText.equals(observedObjectText)) {
						throw new IllegalStateException("text mismatch");
					}

//				System.out.println("sentence span: " + sentenceStartIndex + ".." + sentenceEndIndex);
//				System.out.println("span length: " + (sentenceEndIndex - sentenceStartIndex));
//				System.out.println("sentence text length: " + sentenceText.length());
//				System.out.println("subj span: " + subjStartIndex + ".." + subjEndIndex);
//				System.out.println("obj span: " + objStartIndex + ".." + objEndIndex);
//				System.out.println("segment span: " + segmentStart + ".." + segmentEnd);

//				if (segmentEnd < sentenceText.length()) {

					/*
					 * the segment is the intervening text between the subject and object entities
					 */
					String segmentText = sentenceText.substring(segmentStart, segmentEnd);

					/*
					 * check to see if the subject and object text are expected -- if they are not
					 * then there is a discrepancy between the spans and the sentence
					 */

					/*
					 * we'll count tokens by splitting on spaces and counting the number of tokens
					 * returned.
					 */
					int tokenCount = segmentText.split(" ").length;

					int sentenceTokenCount = sentenceText.split(" ").length;

					String semTypePair = String.format("%s_%s", subjSemType, objSemType);

					String cuiKey = createSubjObjCuiKey(subjectCui, objectCui);
					if (cuiKeyToPmidCount.containsKey(cuiKey)) {
						int pmidCount = cuiKeyToPmidCount.get(cuiKey);
						String pmidCountBucket = getPmidCountBucket(pmidCount);
						String tokenCountBucket = getTokenCountBucket(tokenCount);

						writer.write(String.format("%s,%d,%d,%d,%s,%s,%s\n", predicationId, tokenCount,
								sentenceTokenCount, pmidCount, semTypePair, pmidCountBucket, tokenCountBucket));

						subjSemTypes.add(subjSemType);
						objSemTypes.add(objSemType);

						if (samplePredicationIds == null || samplePredicationIds.contains(predicationId)) {
							String key = createSemTypeKey(subjSemType, objSemType);
							CollectionsUtil.addToCountMap(key, keyToCountMap);
						}

					} else {
						errWriter.write("No cui key for " + cuiKey + "\n");
					}

//				} else {
				} catch (Exception e) {
					errWriter.write(String.format("%s\t%s\n", predicationId, e.getClass().getName()));
				}

			}
		}

		System.out.println("\n\n\n\n");
		printSemTypeDistributionArray(subjSemTypes, objSemTypes, keyToCountMap);

	}

	private static String getTokenCountBucket(int tokenCount) {
		if (tokenCount == 0) {
			return "TOKEN_0";
		}
		if (tokenCount == 1) {
			return "TOKEN_1";
		}
		if (tokenCount >= 2 && tokenCount < 5) {
			return "TOKEN_2_4";
		}
		if (tokenCount >= 5 && tokenCount < 10) {
			return "TOKEN_5_9";
		}
		if (tokenCount >= 10 && tokenCount < 15) {
			return "TOKEN_10_14";
		}

		return "TOKEN_15";
	}

	/**
	 * Based on looking at the distribution of PMID counts, we create some (somewhat
	 * arbitrary) buckets to categorize the data
	 * 
	 * @param pmidCount
	 * @return
	 */
	private static String getPmidCountBucket(int pmidCount) {
		if (pmidCount == 1) {
			return "PMID_1";
		}
		if (pmidCount > 1 && pmidCount < 5) {
			return "PMID_2_4";
		}
		if (pmidCount >= 5 && pmidCount < 10) {
			return "PMID_5_9";
		}
		if (pmidCount >= 10 && pmidCount < 20) {
			return "PMID_10_19";
		}

		return "PMID_20";

	}

	private static Map<String, String> getSemanticTypeMap() {
		Map<String, String> map = new HashMap<String, String>();

		map.put("aapp", "Amino Acid, Peptide, or Protein");
		map.put("antb", "Antibiotic");
		map.put("bacs", "Biologically Active Substance");
		map.put("bodm", "Biomedical or Dental Material");
		map.put("chem", "Chemical");
		map.put("chvf", "Chemical Viewed Functionally");
		map.put("chvs", "Chemical Viewed Structurally");
		map.put("clnd", "Clinical Drug");
		map.put("elii", "Element, Ion, or Isotope");
		map.put("enzy", "Enzyme");
		map.put("hops", "Hazardous or Poisonous Substance");
		map.put("horm", "Hormone");
		map.put("imft", "Immunologic Factor");
		map.put("irda", "Indicator, Reagent, or Diagnostic Aid");
		map.put("inch", "Inorganic Chemical");
		map.put("nnon", "Nucleic Acid, Nucleoside, or Nucleotide");
		map.put("orch", "Organic Chemical");
		map.put("phsu", "Pharmacologic Substance");
		map.put("rcpt", "Receptor");
		map.put("vita", "Vitamin");

		map.put("acab", "Acquired Abnormality");
		map.put("anab", "Anatomical Abnormality");
		map.put("comd", "Cell or Molecular Dysfunction");
		map.put("cgab", "Congenital Abnormality");
		map.put("dsyn", "Disease or Syndrome");
		map.put("emod", "Experimental Model of Disease");
		map.put("fndg", "Finding");
		map.put("inpo", "Injury or Poisoning");
		map.put("mobd", "Mental or Behavioral Dysfunction");
		map.put("neop", "Neoplastic Process");
		map.put("patf", "Pathologic Function");
		map.put("sosy", "Sign or Symptom");

		map.put("diap", "Diagnostic Procedure");
		map.put("edac", "Educational Activity");
		map.put("hlca", "Health Care Activity");
		map.put("lbpr", "Laboratory Procedure");
		map.put("mbrt", "Molecular Biology Research Technique");
		map.put("resa", "Research Activity");
		map.put("topp", "Therapeutic or Preventive Procedure");

		map.put("drdd", "Drug Delivery Device");
		map.put("medd", "Medical Device");
		map.put("resd", "Research Device");

		map.put("virs", "Virus");
		map.put("fngs", "Fungus");
		map.put("bact", "Bacterium");

		return map;
	}

	/**
	 * WARNING - CSV reader is broken -- seems to skip lines at some point, unsure
	 * why. See fix above. This method should not longer be used as it's been
	 * incorporated above. Prints the python matrix useful for creating a heatmap
	 * showing counts of the different semantic type pairs -- plug into colab
	 * 
	 * @param inputFile
	 * @throws IOException
	 */
	public static void getSemanticTypeMatrix(File inputFile) throws IOException {

		Set<String> subjSemTypes = new HashSet<String>();
		Set<String> objSemTypes = new HashSet<String>();
		Map<String, Integer> keyToCountMap = new HashMap<String, Integer>();

		Reader r = new InputStreamReader(new GZIPInputStream(new FileInputStream(inputFile)));

		int lineCount = 0;
		try (CSVReader csvReader = new CSVReader(r)) {
			String[] nextRecord;
			while ((nextRecord = csvReader.readNext()) != null) {

				if (lineCount++ % 10000 == 0) {
					System.out.println("progress: " + (lineCount - 1));
				}

				String subjSemType = nextRecord[16];
				String objSemType = nextRecord[20];

				subjSemTypes.add(subjSemType);
				objSemTypes.add(objSemType);

				String key = createSemTypeKey(subjSemType, objSemType);

				CollectionsUtil.addToCountMap(key, keyToCountMap);
//				if (lineCount == 100) {
//					break;
//				}
			}

		}

		printSemTypeDistributionArray(subjSemTypes, objSemTypes, keyToCountMap);
	}

	/**
	 * print python code
	 * 
	 * @param subjSemTypes
	 * @param objSemTypes
	 * @param keyToCountMap
	 */
	private static void printSemTypeDistributionArray(Set<String> subjSemTypes, Set<String> objSemTypes,
			Map<String, Integer> keyToCountMap) {
		List<String> sortedSubjSemTypes = new ArrayList<String>(subjSemTypes);
		Collections.sort(sortedSubjSemTypes);
		List<String> sortedObjSemTypes = new ArrayList<String>(objSemTypes);
		Collections.sort(sortedObjSemTypes);

		System.out.print("subj_sem_types = [");
		for (String semType : sortedSubjSemTypes) {
			System.out.print(String.format("\"%s\", ", getSemanticTypeMap().get(semType)));
		}
		System.out.println("]");
		System.out.print("obj_sem_types = [");
		for (String semType : sortedObjSemTypes) {
			System.out.print(String.format("\"%s\", ", getSemanticTypeMap().get(semType)));
		}
		System.out.println("]");

		System.out.print("treats_pairs = np.array([");

		for (String subjType : sortedSubjSemTypes) {
			System.out.print("[");
			for (String objType : sortedObjSemTypes) {
				String key = createSemTypeKey(subjType, objType);
				int count = 0;
				if (keyToCountMap.containsKey(key)) {
					count = keyToCountMap.get(key);
				}
				System.out.print(count + ", ");
			}
			System.out.println("],");
		}
		System.out.println("])");
	}

	private static String createSemTypeKey(String subjSemType, String objSemType) {
		String key = subjSemType + "_" + objSemType;
		return key;
	}
}
